# StreamHandler Documentation

## Overview

The `streaming.py` module provides a callback handler for real-time token streaming from Large Language Models (LLMs). This enables a better user experience by displaying responses progressively as they're generated, rather than waiting for the complete response.

## Purpose

This handler serves as a bridge between LangChain's LLM streaming capabilities and your application's UI or output mechanism. It:

1. **Captures tokens** as they're generated by the LLM
2. **Forwards tokens** to a custom callback function in real-time
3. **Enables progressive display** of responses in chat interfaces
4. **Improves perceived responsiveness** of AI applications

## Architecture

### Class: `StreamHandler`

Extends LangChain's `BaseCallbackHandler` to provide streaming functionality.

#### Initialization

```python
from streaming import StreamHandler

def handle_token(token: str):
    print(token, end='', flush=True)

handler = StreamHandler(on_token=handle_token)
```

**Parameters**:
- `on_token` (callable): Function that receives each token as it's generated
  - **Signature**: `def on_token(token: str) -> None`
  - **Called**: Once per token during LLM generation
  - **Thread-safe**: Should handle concurrent calls if using async LLMs

**Attributes**:
- `self.on_token`: Stores the callback function for token processing

### Methods

#### `on_llm_new_token(token, **kwargs)`

Callback method automatically invoked by LangChain when a new token is generated.

**Parameters**:
- `token` (str): The newly generated text token
- `**kwargs`: Additional metadata from LangChain (unused but available)
  - Common kwargs: `run_id`, `parent_run_id`, `tags`, `metadata`

**Behavior**:
- Immediately forwards the token to the `on_token` callback
- No return value (callbacks are fire-and-forget)

**Called By**:
- LangChain's LLM execution engine during streaming generation
- Automatically triggered; you never call this directly

## Usage Examples

### 1. Basic Console Streaming

```python
from streaming import StreamHandler
from langchain_openai import ChatOpenAI

def print_token(token: str):
    print(token, end='', flush=True)

# Create handler
handler = StreamHandler(on_token=print_token)

# Use with LangChain LLM
llm = ChatOpenAI(streaming=True, callbacks=[handler])
response = llm.invoke("Write a short poem about AI")
# Tokens appear progressively: "In", " circuits", " deep", ...
```

### 2. Web Application Streaming (FastAPI)

```python
from streaming import StreamHandler
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio

app = FastAPI()

@app.post("/chat/stream")
async def stream_chat(query: str):
    async def token_generator():
        tokens = []
        
        def on_token(token: str):
            tokens.append(token)
        
        handler = StreamHandler(on_token=on_token)
        
        # Use async LLM with handler
        from langchain_openai import ChatOpenAI
        llm = ChatOpenAI(streaming=True, callbacks=[handler])
        await llm.ainvoke(query)
        
        # Yield tokens as they arrive
        for token in tokens:
            yield f"data: {token}\n\n"
            await asyncio.sleep(0)  # Allow other tasks to run
    
    return StreamingResponse(
        token_generator(),
        media_type="text/event-stream"
    )
```

### 3. Gradio Chat Interface

```python
import gradio as gr
from streaming import StreamHandler
from langchain_openai import ChatOpenAI

def chat_with_streaming(message, history):
    response_text = ""
    
    def on_token(token: str):
        nonlocal response_text
        response_text += token
        # Gradio handles progressive updates automatically
    
    handler = StreamHandler(on_token=on_token)
    llm = ChatOpenAI(streaming=True, callbacks=[handler])
    llm.invoke(message)
    
    return response_text

demo = gr.ChatInterface(chat_with_streaming)
demo.launch()
```

### 4. Custom UI Buffer

```python
from streaming import StreamHandler
import time

class UIBuffer:
    def __init__(self):
        self.buffer = []
        self.complete = False
    
    def add_token(self, token: str):
        self.buffer.append(token)
        # Update UI here
        self.render()
    
    def render(self):
        # Custom rendering logic
        print(f"\rCurrent: {''.join(self.buffer)}", end='', flush=True)
    
    def finalize(self):
        self.complete = True
        print("\n[Response complete]")

# Usage
ui = UIBuffer()
handler = StreamHandler(on_token=ui.add_token)

# Use handler with LLM...
# Then finalize
ui.finalize()
```

### 5. Token Accumulation with Statistics

```python
from streaming import StreamHandler
import time

class StatisticsStreamHandler(StreamHandler):
    def __init__(self, on_token):
        super().__init__(on_token)
        self.token_count = 0
        self.start_time = None
        self.tokens_per_second = 0
    
    def on_llm_start(self, *args, **kwargs):
        """Track when generation starts"""
        self.start_time = time.time()
        self.token_count = 0
    
    def on_llm_new_token(self, token, **kwargs):
        """Track tokens and calculate speed"""
        super().on_llm_new_token(token, **kwargs)
        self.token_count += 1
        
        if self.start_time:
            elapsed = time.time() - self.start_time
            self.tokens_per_second = self.token_count / elapsed if elapsed > 0 else 0
    
    def on_llm_end(self, *args, **kwargs):
        """Print statistics when done"""
        print(f"\n\nStatistics:")
        print(f"  Total tokens: {self.token_count}")
        print(f"  Speed: {self.tokens_per_second:.2f} tokens/sec")

# Usage
def print_token(token):
    print(token, end='', flush=True)

handler = StatisticsStreamHandler(on_token=print_token)
```

## Integration with Router System

### Using with `router.py`

```python
from streaming import StreamHandler
from router import execute

def handle_streamed_response(query: str):
    response_buffer = []
    
    def on_token(token: str):
        response_buffer.append(token)
        print(token, end='', flush=True)
    
    handler = StreamHandler(on_token=on_token)
    
    # Execute with streaming
    result, route, scores = execute(
        query,
        callbacks=[handler]  # Pass handler to execute
    )
    
    print(f"\n\nRoute used: {route.value}")
    print(f"Confidence: {scores[route.value]:.2%}")
    
    return ''.join(response_buffer)

# Example usage
response = handle_streamed_response("What's in the Q3 report?")
```

### Multiple Handlers

```python
from streaming import StreamHandler

def console_output(token: str):
    print(token, end='', flush=True)

def file_logger(token: str):
    with open('response_log.txt', 'a') as f:
        f.write(token)

def metrics_tracker(token: str):
    # Track tokens for analytics
    pass

# Create multiple handlers
handlers = [
    StreamHandler(on_token=console_output),
    StreamHandler(on_token=file_logger),
    StreamHandler(on_token=metrics_tracker)
]

# Use all handlers together
from router import execute
result, route, scores = execute(query, callbacks=handlers)
```

## Alternative Approaches

### 1. Server-Sent Events (SSE)

**Pros**:
- Native browser support
- Automatic reconnection
- Simple protocol

**Cons**:
- One-way communication only
- Limited to text data
- Not suitable for binary streams

**Implementation**:
```python
from typing import Generator
from streaming import StreamHandler

def sse_stream(query: str) -> Generator[str, None, None]:
    """Generate SSE-formatted stream"""
    def on_token(token: str):
        # Escape newlines for SSE format
        escaped = token.replace('\n', '\\n')
        yield f"data: {escaped}\n\n"
    
    handler = StreamHandler(on_token=on_token)
    # Use with LLM...
```

### 2. WebSocket Streaming

**Pros**:
- Bi-directional communication
- Lower latency than SSE
- Binary data support

**Cons**:
- More complex setup
- Requires WebSocket server
- Connection management overhead

**Implementation**:
```python
from fastapi import WebSocket
from streaming import StreamHandler

async def websocket_stream(websocket: WebSocket, query: str):
    await websocket.accept()
    
    async def on_token(token: str):
        await websocket.send_text(token)
    
    handler = StreamHandler(on_token=on_token)
    # Use with async LLM...
    
    await websocket.close()
```

### 3. Custom Callback Handler

**Pros**:
- Full control over behavior
- Can add custom logic
- No dependency on StreamHandler

**Cons**:
- More code to maintain
- Must understand LangChain callback system

**Implementation**:
```python
from langchain_core.callbacks import BaseCallbackHandler
from typing import Any, Dict, List

class CustomStreamHandler(BaseCallbackHandler):
    def __init__(self):
        self.tokens = []
        self.metadata = {}
    
    def on_llm_start(
        self,
        serialized: Dict[str, Any],
        prompts: List[str],
        **kwargs
    ):
        """Called when LLM starts"""
        self.metadata['start_time'] = time.time()
        print(f"Starting LLM with prompts: {prompts}")
    
    def on_llm_new_token(self, token: str, **kwargs):
        """Called for each new token"""
        self.tokens.append(token)
        # Custom processing
        if token.strip():  # Only print non-whitespace
            print(token, end='', flush=True)
    
    def on_llm_end(self, response, **kwargs):
        """Called when LLM finishes"""
        self.metadata['end_time'] = time.time()
        duration = self.metadata['end_time'] - self.metadata['start_time']
        print(f"\n\nCompleted in {duration:.2f}s")
    
    def on_llm_error(self, error: Exception, **kwargs):
        """Called on LLM error"""
        print(f"\nError occurred: {error}")
```

### 4. Generator-Based Streaming

**Pros**:
- Pythonic approach
- Works well with async/await
- Easy to understand control flow

**Cons**:
- Requires LLM to support generator interface
- May not work with all LangChain components

**Implementation**:
```python
def stream_response(query: str):
    """Generator that yields tokens"""
    from langchain_openai import ChatOpenAI
    
    llm = ChatOpenAI(streaming=True)
    
    for chunk in llm.stream(query):
        if hasattr(chunk, 'content'):
            yield chunk.content
        else:
            yield str(chunk)

# Usage
for token in stream_response("Hello, AI!"):
    print(token, end='', flush=True)
```

### 5. Queue-Based Streaming

**Pros**:
- Thread-safe
- Decouples producer and consumer
- Good for async scenarios

**Cons**:
- More complex
- Additional memory overhead
- Requires queue management

**Implementation**:
```python
import queue
import threading
from streaming import StreamHandler

class QueueStreamHandler(StreamHandler):
    def __init__(self):
        self.token_queue = queue.Queue()
        super().__init__(on_token=self._enqueue_token)
    
    def _enqueue_token(self, token: str):
        self.token_queue.put(token)
    
    def get_stream(self):
        """Generator that yields from queue"""
        while True:
            try:
                token = self.token_queue.get(timeout=0.1)
                if token is None:  # Sentinel value
                    break
                yield token
            except queue.Empty:
                continue

# Usage
handler = QueueStreamHandler()
# Start LLM in thread...
for token in handler.get_stream():
    print(token, end='')
```

## Performance Considerations

### 1. Buffering Strategies

**Micro-buffering** (current implementation):
- Forwards each token immediately
- Lowest latency
- Higher overhead per token

**Batch buffering**:
```python
class BatchStreamHandler(StreamHandler):
    def __init__(self, on_token, batch_size=5):
        super().__init__(on_token)
        self.batch_size = batch_size
        self.buffer = []
    
    def on_llm_new_token(self, token, **kwargs):
        self.buffer.append(token)
        if len(self.buffer) >= self.batch_size:
            self.flush()
    
    def flush(self):
        if self.buffer:
            combined = ''.join(self.buffer)
            self.on_token(combined)
            self.buffer = []
    
    def on_llm_end(self, *args, **kwargs):
        self.flush()  # Send remaining tokens
```

### 2. Network Optimization

**For web applications**:
- Enable compression (gzip)
- Use HTTP/2 for multiplexing
- Implement client-side buffering

```python
# FastAPI with compression
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(GZipMiddleware, minimum_size=1000)
```

### 3. Memory Management

**For long responses**:
```python
class MemoryEfficientHandler(StreamHandler):
    def __init__(self, on_token, max_buffer_size=1000):
        super().__init__(on_token)
        self.max_buffer_size = max_buffer_size
        self.token_count = 0
    
    def on_llm_new_token(self, token, **kwargs):
        super().on_llm_new_token(token, **kwargs)
        self.token_count += 1
        
        # Periodically clear resources
        if self.token_count % self.max_buffer_size == 0:
            import gc
            gc.collect()
```

## Error Handling

### Robust Token Handler

```python
from streaming import StreamHandler
import logging

logger = logging.getLogger(__name__)

def create_safe_handler(on_token):
    """Create handler with error handling"""
    
    def safe_token_callback(token: str):
        try:
            on_token(token)
        except Exception as e:
            logger.error(f"Error in token callback: {e}")
            # Continue streaming despite errors
    
    return StreamHandler(on_token=safe_token_callback)

# Usage
def my_token_handler(token):
    # This might fail
    risky_operation(token)

handler = create_safe_handler(my_token_handler)
```

### Connection Failure Recovery

```python
class ResilientStreamHandler(StreamHandler):
    def __init__(self, on_token, max_retries=3):
        super().__init__(on_token)
        self.max_retries = max_retries
        self.retry_count = 0
    
    def on_llm_new_token(self, token, **kwargs):
        for attempt in range(self.max_retries):
            try:
                self.on_token(token)
                self.retry_count = 0  # Reset on success
                break
            except ConnectionError:
                self.retry_count += 1
                if attempt == self.max_retries - 1:
                    logger.error("Max retries reached, dropping token")
                else:
                    time.sleep(0.1 * (attempt + 1))  # Exponential backoff
```

## Testing

### Unit Tests

```python
import unittest
from streaming import StreamHandler

class TestStreamHandler(unittest.TestCase):
    def test_token_callback_called(self):
        """Test that callback is invoked for each token"""
        tokens_received = []
        
        def callback(token):
            tokens_received.append(token)
        
        handler = StreamHandler(on_token=callback)
        
        # Simulate token streaming
        handler.on_llm_new_token("Hello")
        handler.on_llm_new_token(" ")
        handler.on_llm_new_token("World")
        
        self.assertEqual(tokens_received, ["Hello", " ", "World"])
    
    def test_callback_receives_correct_token(self):
        """Test that exact token is passed to callback"""
        received_token = None
        
        def callback(token):
            nonlocal received_token
            received_token = token
        
        handler = StreamHandler(on_token=callback)
        handler.on_llm_new_token("test_token")
        
        self.assertEqual(received_token, "test_token")
```

### Integration Tests

```python
def test_streaming_with_llm():
    """Test streaming with actual LLM"""
    from langchain_openai import ChatOpenAI
    
    tokens = []
    handler = StreamHandler(on_token=tokens.append)
    
    llm = ChatOpenAI(
        streaming=True,
        callbacks=[handler],
        temperature=0  # Deterministic for testing
    )
    
    response = llm.invoke("Say 'test'")
    
    # Verify tokens were received
    assert len(tokens) > 0
    assert ''.join(tokens) == response.content
```

### Mock Testing

```python
from unittest.mock import Mock, call

def test_handler_with_mock():
    """Test handler behavior with mocked callback"""
    mock_callback = Mock()
    handler = StreamHandler(on_token=mock_callback)
    
    # Simulate token stream
    test_tokens = ["Hello", " ", "World", "!"]
    for token in test_tokens:
        handler.on_llm_new_token(token)
    
    # Verify all tokens were passed
    assert mock_callback.call_count == len(test_tokens)
    mock_callback.assert_has_calls([call(t) for t in test_tokens])
```

## Best Practices

### 1. Callback Design

✅ **DO**:
- Keep callbacks lightweight and fast
- Use async callbacks for I/O operations
- Handle exceptions within callbacks
- Make callbacks idempotent when possible

❌ **DON'T**:
- Perform heavy computation in callbacks
- Block on network calls in sync callbacks
- Raise exceptions without handling
- Modify shared state without locks

### 2. User Experience

```python
class UIFriendlyHandler(StreamHandler):
    def __init__(self, on_token, typing_indicator=True):
        super().__init__(on_token)
        self.typing_indicator = typing_indicator
        self.first_token = True
    
    def on_llm_start(self, *args, **kwargs):
        if self.typing_indicator:
            self.on_token("⋯")  # Show typing indicator
    
    def on_llm_new_token(self, token, **kwargs):
        if self.first_token and self.typing_indicator:
            # Clear typing indicator
            self.on_token("\b\b\b")
            self.first_token = False
        
        super().on_llm_new_token(token, **kwargs)
```

### 3. Resource Cleanup

```python
class ManagedStreamHandler(StreamHandler):
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # Cleanup resources
        if hasattr(self, 'file_handle'):
            self.file_handle.close()
        return False

# Usage
with ManagedStreamHandler(on_token=logger) as handler:
    # Use handler
    pass
# Automatic cleanup
```

## Monitoring & Debugging

### Debug Handler

```python
class DebugStreamHandler(StreamHandler):
    def __init__(self, on_token, verbose=True):
        super().__init__(on_token)
        self.verbose = verbose
        self.token_log = []
    
    def on_llm_new_token(self, token, **kwargs):
        if self.verbose:
            # Log metadata
            print(f"\n[Token {len(self.token_log)}]: '{token}'")
            print(f"  Kwargs: {kwargs}")
        
        self.token_log.append({
            'token': token,
            'timestamp': time.time(),
            'metadata': kwargs
        })
        
        super().on_llm_new_token(token, **kwargs)
    
    def get_stats(self):
        return {
            'total_tokens': len(self.token_log),
            'avg_token_length': sum(len(t['token']) for t in self.token_log) / len(self.token_log),
            'duration': self.token_log[-1]['timestamp'] - self.token_log[0]['timestamp']
        }
```

## Conclusion

The `StreamHandler` class provides a simple yet powerful interface for real-time token streaming. It's ideal for:

- **Chat interfaces** requiring progressive response display
- **CLI applications** showing generation progress
- **Web applications** using SSE or WebSocket
- **Analytics pipelines** processing tokens in real-time

### When to Use StreamHandler

✅ Use when:
- Building interactive chat UIs
- Users expect real-time feedback
- Response generation is slow (>2 seconds)
- You need to process tokens incrementally

❌ Consider alternatives when:
- Responses are very short (<10 tokens)
- Batch processing is sufficient
- Streaming adds unnecessary complexity
- Network conditions are unreliable

The minimal implementation (just 9 lines of code) makes it easy to understand, extend, and integrate into any LangChain-based application.